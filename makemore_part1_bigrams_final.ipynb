{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wUk04TAUi459"
      },
      "outputs": [],
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA3CTYwBi45_",
        "outputId": "7db2183a-685c-431f-8eff-439e2742e310"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "words[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qwxxAcNi46A",
        "outputId": "dce90103-959b-423c-f6e3-472b0e0e1ae9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3EPO5Y6i46B",
        "outputId": "40df31f6-6da0-461e-b61a-7e61c01f33b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "min(len(w) for w in words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Dh4CQni46B",
        "outputId": "af0ad54a-44cc-4e0a-ed1e-c6aae7489339"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "max(len(w) for w in words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Rjnjw_Xgi46C"
      },
      "outputs": [],
      "source": [
        "b = {}\n",
        "for w in words:\n",
        "  chs = ['<S>'] + list(w) + ['<E>']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    bigram = (ch1, ch2)\n",
        "    b[bigram] = b.get(bigram, 0) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re1yKToKi46C",
        "outputId": "df573f9e-c326-4136-d72d-1647910f35db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('n', '<E>'), 6763),\n",
              " (('a', '<E>'), 6640),\n",
              " (('a', 'n'), 5438),\n",
              " (('<S>', 'a'), 4410),\n",
              " (('e', '<E>'), 3983),\n",
              " (('a', 'r'), 3264),\n",
              " (('e', 'l'), 3248),\n",
              " (('r', 'i'), 3033),\n",
              " (('n', 'a'), 2977),\n",
              " (('<S>', 'k'), 2963),\n",
              " (('l', 'e'), 2921),\n",
              " (('e', 'n'), 2675),\n",
              " (('l', 'a'), 2623),\n",
              " (('m', 'a'), 2590),\n",
              " (('<S>', 'm'), 2538),\n",
              " (('a', 'l'), 2528),\n",
              " (('i', '<E>'), 2489),\n",
              " (('l', 'i'), 2480),\n",
              " (('i', 'a'), 2445),\n",
              " (('<S>', 'j'), 2422),\n",
              " (('o', 'n'), 2411),\n",
              " (('h', '<E>'), 2409),\n",
              " (('r', 'a'), 2356),\n",
              " (('a', 'h'), 2332),\n",
              " (('h', 'a'), 2244),\n",
              " (('y', 'a'), 2143),\n",
              " (('i', 'n'), 2126),\n",
              " (('<S>', 's'), 2055),\n",
              " (('a', 'y'), 2050),\n",
              " (('y', '<E>'), 2007),\n",
              " (('e', 'r'), 1958),\n",
              " (('n', 'n'), 1906),\n",
              " (('y', 'n'), 1826),\n",
              " (('k', 'a'), 1731),\n",
              " (('n', 'i'), 1725),\n",
              " (('r', 'e'), 1697),\n",
              " (('<S>', 'd'), 1690),\n",
              " (('i', 'e'), 1653),\n",
              " (('a', 'i'), 1650),\n",
              " (('<S>', 'r'), 1639),\n",
              " (('a', 'm'), 1634),\n",
              " (('l', 'y'), 1588),\n",
              " (('<S>', 'l'), 1572),\n",
              " (('<S>', 'c'), 1542),\n",
              " (('<S>', 'e'), 1531),\n",
              " (('j', 'a'), 1473),\n",
              " (('r', '<E>'), 1377),\n",
              " (('n', 'e'), 1359),\n",
              " (('l', 'l'), 1345),\n",
              " (('i', 'l'), 1345),\n",
              " (('i', 's'), 1316),\n",
              " (('l', '<E>'), 1314),\n",
              " (('<S>', 't'), 1308),\n",
              " (('<S>', 'b'), 1306),\n",
              " (('d', 'a'), 1303),\n",
              " (('s', 'h'), 1285),\n",
              " (('d', 'e'), 1283),\n",
              " (('e', 'e'), 1271),\n",
              " (('m', 'i'), 1256),\n",
              " (('s', 'a'), 1201),\n",
              " (('s', '<E>'), 1169),\n",
              " (('<S>', 'n'), 1146),\n",
              " (('a', 's'), 1118),\n",
              " (('y', 'l'), 1104),\n",
              " (('e', 'y'), 1070),\n",
              " (('o', 'r'), 1059),\n",
              " (('a', 'd'), 1042),\n",
              " (('t', 'a'), 1027),\n",
              " (('<S>', 'z'), 929),\n",
              " (('v', 'i'), 911),\n",
              " (('k', 'e'), 895),\n",
              " (('s', 'e'), 884),\n",
              " (('<S>', 'h'), 874),\n",
              " (('r', 'o'), 869),\n",
              " (('e', 's'), 861),\n",
              " (('z', 'a'), 860),\n",
              " (('o', '<E>'), 855),\n",
              " (('i', 'r'), 849),\n",
              " (('b', 'r'), 842),\n",
              " (('a', 'v'), 834),\n",
              " (('m', 'e'), 818),\n",
              " (('e', 'i'), 818),\n",
              " (('c', 'a'), 815),\n",
              " (('i', 'y'), 779),\n",
              " (('r', 'y'), 773),\n",
              " (('e', 'm'), 769),\n",
              " (('s', 't'), 765),\n",
              " (('h', 'i'), 729),\n",
              " (('t', 'e'), 716),\n",
              " (('n', 'd'), 704),\n",
              " (('l', 'o'), 692),\n",
              " (('a', 'e'), 692),\n",
              " (('a', 't'), 687),\n",
              " (('s', 'i'), 684),\n",
              " (('e', 'a'), 679),\n",
              " (('d', 'i'), 674),\n",
              " (('h', 'e'), 674),\n",
              " (('<S>', 'g'), 669),\n",
              " (('t', 'o'), 667),\n",
              " (('c', 'h'), 664),\n",
              " (('b', 'e'), 655),\n",
              " (('t', 'h'), 647),\n",
              " (('v', 'a'), 642),\n",
              " (('o', 'l'), 619),\n",
              " (('<S>', 'i'), 591),\n",
              " (('i', 'o'), 588),\n",
              " (('e', 't'), 580),\n",
              " (('v', 'e'), 568),\n",
              " (('a', 'k'), 568),\n",
              " (('a', 'a'), 556),\n",
              " (('c', 'e'), 551),\n",
              " (('a', 'b'), 541),\n",
              " (('i', 't'), 541),\n",
              " (('<S>', 'y'), 535),\n",
              " (('t', 'i'), 532),\n",
              " (('s', 'o'), 531),\n",
              " (('m', '<E>'), 516),\n",
              " (('d', '<E>'), 516),\n",
              " (('<S>', 'p'), 515),\n",
              " (('i', 'c'), 509),\n",
              " (('k', 'i'), 509),\n",
              " (('o', 's'), 504),\n",
              " (('n', 'o'), 496),\n",
              " (('t', '<E>'), 483),\n",
              " (('j', 'o'), 479),\n",
              " (('u', 's'), 474),\n",
              " (('a', 'c'), 470),\n",
              " (('n', 'y'), 465),\n",
              " (('e', 'v'), 463),\n",
              " (('s', 's'), 461),\n",
              " (('m', 'o'), 452),\n",
              " (('i', 'k'), 445),\n",
              " (('n', 't'), 443),\n",
              " (('i', 'd'), 440),\n",
              " (('j', 'e'), 440),\n",
              " (('a', 'z'), 435),\n",
              " (('i', 'g'), 428),\n",
              " (('i', 'm'), 427),\n",
              " (('r', 'r'), 425),\n",
              " (('d', 'r'), 424),\n",
              " (('<S>', 'f'), 417),\n",
              " (('u', 'r'), 414),\n",
              " (('r', 'l'), 413),\n",
              " (('y', 's'), 401),\n",
              " (('<S>', 'o'), 394),\n",
              " (('e', 'd'), 384),\n",
              " (('a', 'u'), 381),\n",
              " (('c', 'o'), 380),\n",
              " (('k', 'y'), 379),\n",
              " (('d', 'o'), 378),\n",
              " (('<S>', 'v'), 376),\n",
              " (('t', 't'), 374),\n",
              " (('z', 'e'), 373),\n",
              " (('z', 'i'), 364),\n",
              " (('k', '<E>'), 363),\n",
              " (('g', 'h'), 360),\n",
              " (('t', 'r'), 352),\n",
              " (('k', 'o'), 344),\n",
              " (('t', 'y'), 341),\n",
              " (('g', 'e'), 334),\n",
              " (('g', 'a'), 330),\n",
              " (('l', 'u'), 324),\n",
              " (('b', 'a'), 321),\n",
              " (('d', 'y'), 317),\n",
              " (('c', 'k'), 316),\n",
              " (('<S>', 'w'), 307),\n",
              " (('k', 'h'), 307),\n",
              " (('u', 'l'), 301),\n",
              " (('y', 'e'), 301),\n",
              " (('y', 'r'), 291),\n",
              " (('m', 'y'), 287),\n",
              " (('h', 'o'), 287),\n",
              " (('w', 'a'), 280),\n",
              " (('s', 'l'), 279),\n",
              " (('n', 's'), 278),\n",
              " (('i', 'z'), 277),\n",
              " (('u', 'n'), 275),\n",
              " (('o', 'u'), 275),\n",
              " (('n', 'g'), 273),\n",
              " (('y', 'd'), 272),\n",
              " (('c', 'i'), 271),\n",
              " (('y', 'o'), 271),\n",
              " (('i', 'v'), 269),\n",
              " (('e', 'o'), 269),\n",
              " (('o', 'm'), 261),\n",
              " (('r', 'u'), 252),\n",
              " (('f', 'a'), 242),\n",
              " (('b', 'i'), 217),\n",
              " (('s', 'y'), 215),\n",
              " (('n', 'c'), 213),\n",
              " (('h', 'y'), 213),\n",
              " (('p', 'a'), 209),\n",
              " (('r', 't'), 208),\n",
              " (('q', 'u'), 206),\n",
              " (('p', 'h'), 204),\n",
              " (('h', 'r'), 204),\n",
              " (('j', 'u'), 202),\n",
              " (('g', 'r'), 201),\n",
              " (('p', 'e'), 197),\n",
              " (('n', 'l'), 195),\n",
              " (('y', 'i'), 192),\n",
              " (('g', 'i'), 190),\n",
              " (('o', 'd'), 190),\n",
              " (('r', 's'), 190),\n",
              " (('r', 'd'), 187),\n",
              " (('h', 'l'), 185),\n",
              " (('s', 'u'), 185),\n",
              " (('a', 'x'), 182),\n",
              " (('e', 'z'), 181),\n",
              " (('e', 'k'), 178),\n",
              " (('o', 'v'), 176),\n",
              " (('a', 'j'), 175),\n",
              " (('o', 'h'), 171),\n",
              " (('u', 'e'), 169),\n",
              " (('m', 'm'), 168),\n",
              " (('a', 'g'), 168),\n",
              " (('h', 'u'), 166),\n",
              " (('x', '<E>'), 164),\n",
              " (('u', 'a'), 163),\n",
              " (('r', 'm'), 162),\n",
              " (('a', 'w'), 161),\n",
              " (('f', 'i'), 160),\n",
              " (('z', '<E>'), 160),\n",
              " (('u', '<E>'), 155),\n",
              " (('u', 'm'), 154),\n",
              " (('e', 'c'), 153),\n",
              " (('v', 'o'), 153),\n",
              " (('e', 'h'), 152),\n",
              " (('p', 'r'), 151),\n",
              " (('d', 'd'), 149),\n",
              " (('o', 'a'), 149),\n",
              " (('w', 'e'), 149),\n",
              " (('w', 'i'), 148),\n",
              " (('y', 'm'), 148),\n",
              " (('z', 'y'), 147),\n",
              " (('n', 'z'), 145),\n",
              " (('y', 'u'), 141),\n",
              " (('r', 'n'), 140),\n",
              " (('o', 'b'), 140),\n",
              " (('k', 'l'), 139),\n",
              " (('m', 'u'), 139),\n",
              " (('l', 'd'), 138),\n",
              " (('h', 'n'), 138),\n",
              " (('u', 'd'), 136),\n",
              " (('<S>', 'x'), 134),\n",
              " (('t', 'l'), 134),\n",
              " (('a', 'f'), 134),\n",
              " (('o', 'e'), 132),\n",
              " (('e', 'x'), 132),\n",
              " (('e', 'g'), 125),\n",
              " (('f', 'e'), 123),\n",
              " (('z', 'l'), 123),\n",
              " (('u', 'i'), 121),\n",
              " (('v', 'y'), 121),\n",
              " (('e', 'b'), 121),\n",
              " (('r', 'h'), 121),\n",
              " (('j', 'i'), 119),\n",
              " (('o', 't'), 118),\n",
              " (('d', 'h'), 118),\n",
              " (('h', 'm'), 117),\n",
              " (('c', 'l'), 116),\n",
              " (('o', 'o'), 115),\n",
              " (('y', 'c'), 115),\n",
              " (('o', 'w'), 114),\n",
              " (('o', 'c'), 114),\n",
              " (('f', 'r'), 114),\n",
              " (('b', '<E>'), 114),\n",
              " (('m', 'b'), 112),\n",
              " (('z', 'o'), 110),\n",
              " (('i', 'b'), 110),\n",
              " (('i', 'u'), 109),\n",
              " (('k', 'r'), 109),\n",
              " (('g', '<E>'), 108),\n",
              " (('y', 'v'), 106),\n",
              " (('t', 'z'), 105),\n",
              " (('b', 'o'), 105),\n",
              " (('c', 'y'), 104),\n",
              " (('y', 't'), 104),\n",
              " (('u', 'b'), 103),\n",
              " (('u', 'c'), 103),\n",
              " (('x', 'a'), 103),\n",
              " (('b', 'l'), 103),\n",
              " (('o', 'y'), 103),\n",
              " (('x', 'i'), 102),\n",
              " (('i', 'f'), 101),\n",
              " (('r', 'c'), 99),\n",
              " (('c', '<E>'), 97),\n",
              " (('m', 'r'), 97),\n",
              " (('n', 'u'), 96),\n",
              " (('o', 'p'), 95),\n",
              " (('i', 'h'), 95),\n",
              " (('k', 's'), 95),\n",
              " (('l', 's'), 94),\n",
              " (('u', 'k'), 93),\n",
              " (('<S>', 'q'), 92),\n",
              " (('d', 'u'), 92),\n",
              " (('s', 'm'), 90),\n",
              " (('r', 'k'), 90),\n",
              " (('i', 'x'), 89),\n",
              " (('v', '<E>'), 88),\n",
              " (('y', 'k'), 86),\n",
              " (('u', 'w'), 86),\n",
              " (('g', 'u'), 85),\n",
              " (('b', 'y'), 83),\n",
              " (('e', 'p'), 83),\n",
              " (('g', 'o'), 83),\n",
              " (('s', 'k'), 82),\n",
              " (('u', 't'), 82),\n",
              " (('a', 'p'), 82),\n",
              " (('e', 'f'), 82),\n",
              " (('i', 'i'), 82),\n",
              " (('r', 'v'), 80),\n",
              " (('f', '<E>'), 80),\n",
              " (('t', 'u'), 78),\n",
              " (('y', 'z'), 78),\n",
              " (('<S>', 'u'), 78),\n",
              " (('l', 't'), 77),\n",
              " (('r', 'g'), 76),\n",
              " (('c', 'r'), 76),\n",
              " (('i', 'j'), 76),\n",
              " (('w', 'y'), 73),\n",
              " (('z', 'u'), 73),\n",
              " (('l', 'v'), 72),\n",
              " (('h', 't'), 71),\n",
              " (('j', '<E>'), 71),\n",
              " (('x', 't'), 70),\n",
              " (('o', 'i'), 69),\n",
              " (('e', 'u'), 69),\n",
              " (('o', 'k'), 68),\n",
              " (('b', 'd'), 65),\n",
              " (('a', 'o'), 63),\n",
              " (('p', 'i'), 61),\n",
              " (('s', 'c'), 60),\n",
              " (('d', 'l'), 60),\n",
              " (('l', 'm'), 60),\n",
              " (('a', 'q'), 60),\n",
              " (('f', 'o'), 60),\n",
              " (('p', 'o'), 59),\n",
              " (('n', 'k'), 58),\n",
              " (('w', 'n'), 58),\n",
              " (('u', 'h'), 58),\n",
              " (('e', 'j'), 55),\n",
              " (('n', 'v'), 55),\n",
              " (('s', 'r'), 55),\n",
              " (('o', 'z'), 54),\n",
              " (('i', 'p'), 53),\n",
              " (('l', 'b'), 52),\n",
              " (('i', 'q'), 52),\n",
              " (('w', '<E>'), 51),\n",
              " (('m', 'c'), 51),\n",
              " (('s', 'p'), 51),\n",
              " (('e', 'w'), 50),\n",
              " (('k', 'u'), 50),\n",
              " (('v', 'r'), 48),\n",
              " (('u', 'g'), 47),\n",
              " (('o', 'x'), 45),\n",
              " (('u', 'z'), 45),\n",
              " (('z', 'z'), 45),\n",
              " (('j', 'h'), 45),\n",
              " (('b', 'u'), 45),\n",
              " (('o', 'g'), 44),\n",
              " (('n', 'r'), 44),\n",
              " (('f', 'f'), 44),\n",
              " (('n', 'j'), 44),\n",
              " (('z', 'h'), 43),\n",
              " (('c', 'c'), 42),\n",
              " (('r', 'b'), 41),\n",
              " (('x', 'o'), 41),\n",
              " (('b', 'h'), 41),\n",
              " (('p', 'p'), 39),\n",
              " (('x', 'l'), 39),\n",
              " (('h', 'v'), 39),\n",
              " (('b', 'b'), 38),\n",
              " (('m', 'p'), 38),\n",
              " (('x', 'x'), 38),\n",
              " (('u', 'v'), 37),\n",
              " (('x', 'e'), 36),\n",
              " (('w', 'o'), 36),\n",
              " (('c', 't'), 35),\n",
              " (('z', 'm'), 35),\n",
              " (('t', 's'), 35),\n",
              " (('m', 's'), 35),\n",
              " (('c', 'u'), 35),\n",
              " (('o', 'f'), 34),\n",
              " (('u', 'x'), 34),\n",
              " (('k', 'w'), 34),\n",
              " (('p', '<E>'), 33),\n",
              " (('g', 'l'), 32),\n",
              " (('z', 'r'), 32),\n",
              " (('d', 'n'), 31),\n",
              " (('g', 't'), 31),\n",
              " (('g', 'y'), 31),\n",
              " (('h', 's'), 31),\n",
              " (('x', 's'), 31),\n",
              " (('g', 's'), 30),\n",
              " (('x', 'y'), 30),\n",
              " (('y', 'g'), 30),\n",
              " (('d', 'm'), 30),\n",
              " (('d', 's'), 29),\n",
              " (('h', 'k'), 29),\n",
              " (('y', 'x'), 28),\n",
              " (('q', '<E>'), 28),\n",
              " (('g', 'n'), 27),\n",
              " (('y', 'b'), 27),\n",
              " (('g', 'w'), 26),\n",
              " (('n', 'h'), 26),\n",
              " (('k', 'n'), 26),\n",
              " (('g', 'g'), 25),\n",
              " (('d', 'g'), 25),\n",
              " (('l', 'c'), 25),\n",
              " (('r', 'j'), 25),\n",
              " (('w', 'u'), 25),\n",
              " (('l', 'k'), 24),\n",
              " (('m', 'd'), 24),\n",
              " (('s', 'w'), 24),\n",
              " (('s', 'n'), 24),\n",
              " (('h', 'd'), 24),\n",
              " (('w', 'h'), 23),\n",
              " (('y', 'j'), 23),\n",
              " (('y', 'y'), 23),\n",
              " (('r', 'z'), 23),\n",
              " (('d', 'w'), 23),\n",
              " (('w', 'r'), 22),\n",
              " (('t', 'n'), 22),\n",
              " (('l', 'f'), 22),\n",
              " (('y', 'h'), 22),\n",
              " (('r', 'w'), 21),\n",
              " (('s', 'b'), 21),\n",
              " (('m', 'n'), 20),\n",
              " (('f', 'l'), 20),\n",
              " (('w', 's'), 20),\n",
              " (('k', 'k'), 20),\n",
              " (('h', 'z'), 20),\n",
              " (('g', 'd'), 19),\n",
              " (('l', 'h'), 19),\n",
              " (('n', 'm'), 19),\n",
              " (('x', 'z'), 19),\n",
              " (('u', 'f'), 19),\n",
              " (('f', 't'), 18),\n",
              " (('l', 'r'), 18),\n",
              " (('p', 't'), 17),\n",
              " (('t', 'c'), 17),\n",
              " (('k', 't'), 17),\n",
              " (('d', 'v'), 17),\n",
              " (('u', 'p'), 16),\n",
              " (('p', 'l'), 16),\n",
              " (('l', 'w'), 16),\n",
              " (('p', 's'), 16),\n",
              " (('o', 'j'), 16),\n",
              " (('r', 'q'), 16),\n",
              " (('y', 'p'), 15),\n",
              " (('l', 'p'), 15),\n",
              " (('t', 'v'), 15),\n",
              " (('r', 'p'), 14),\n",
              " (('l', 'n'), 14),\n",
              " (('e', 'q'), 14),\n",
              " (('f', 'y'), 14),\n",
              " (('s', 'v'), 14),\n",
              " (('u', 'j'), 14),\n",
              " (('v', 'l'), 14),\n",
              " (('q', 'a'), 13),\n",
              " (('u', 'y'), 13),\n",
              " (('q', 'i'), 13),\n",
              " (('w', 'l'), 13),\n",
              " (('p', 'y'), 12),\n",
              " (('y', 'f'), 12),\n",
              " (('c', 'q'), 11),\n",
              " (('j', 'r'), 11),\n",
              " (('n', 'w'), 11),\n",
              " (('n', 'f'), 11),\n",
              " (('t', 'w'), 11),\n",
              " (('m', 'z'), 11),\n",
              " (('u', 'o'), 10),\n",
              " (('f', 'u'), 10),\n",
              " (('l', 'z'), 10),\n",
              " (('h', 'w'), 10),\n",
              " (('u', 'q'), 10),\n",
              " (('j', 'y'), 10),\n",
              " (('s', 'z'), 10),\n",
              " (('s', 'd'), 9),\n",
              " (('j', 'l'), 9),\n",
              " (('d', 'j'), 9),\n",
              " (('k', 'm'), 9),\n",
              " (('r', 'f'), 9),\n",
              " (('h', 'j'), 9),\n",
              " (('v', 'n'), 8),\n",
              " (('n', 'b'), 8),\n",
              " (('i', 'w'), 8),\n",
              " (('h', 'b'), 8),\n",
              " (('b', 's'), 8),\n",
              " (('w', 't'), 8),\n",
              " (('w', 'd'), 8),\n",
              " (('v', 'v'), 7),\n",
              " (('v', 'u'), 7),\n",
              " (('j', 's'), 7),\n",
              " (('m', 'j'), 7),\n",
              " (('f', 's'), 6),\n",
              " (('l', 'g'), 6),\n",
              " (('l', 'j'), 6),\n",
              " (('j', 'w'), 6),\n",
              " (('n', 'x'), 6),\n",
              " (('y', 'q'), 6),\n",
              " (('w', 'k'), 6),\n",
              " (('g', 'm'), 6),\n",
              " (('x', 'u'), 5),\n",
              " (('m', 'h'), 5),\n",
              " (('m', 'l'), 5),\n",
              " (('j', 'm'), 5),\n",
              " (('c', 's'), 5),\n",
              " (('j', 'v'), 5),\n",
              " (('n', 'p'), 5),\n",
              " (('d', 'f'), 5),\n",
              " (('x', 'd'), 5),\n",
              " (('z', 'b'), 4),\n",
              " (('f', 'n'), 4),\n",
              " (('x', 'c'), 4),\n",
              " (('m', 't'), 4),\n",
              " (('t', 'm'), 4),\n",
              " (('z', 'n'), 4),\n",
              " (('z', 't'), 4),\n",
              " (('p', 'u'), 4),\n",
              " (('c', 'z'), 4),\n",
              " (('b', 'n'), 4),\n",
              " (('z', 's'), 4),\n",
              " (('f', 'w'), 4),\n",
              " (('d', 't'), 4),\n",
              " (('j', 'd'), 4),\n",
              " (('j', 'c'), 4),\n",
              " (('y', 'w'), 4),\n",
              " (('v', 'k'), 3),\n",
              " (('x', 'w'), 3),\n",
              " (('t', 'j'), 3),\n",
              " (('c', 'j'), 3),\n",
              " (('q', 'w'), 3),\n",
              " (('g', 'b'), 3),\n",
              " (('o', 'q'), 3),\n",
              " (('r', 'x'), 3),\n",
              " (('d', 'c'), 3),\n",
              " (('g', 'j'), 3),\n",
              " (('x', 'f'), 3),\n",
              " (('z', 'w'), 3),\n",
              " (('d', 'k'), 3),\n",
              " (('u', 'u'), 3),\n",
              " (('m', 'v'), 3),\n",
              " (('c', 'x'), 3),\n",
              " (('l', 'q'), 3),\n",
              " (('p', 'b'), 2),\n",
              " (('t', 'g'), 2),\n",
              " (('q', 's'), 2),\n",
              " (('t', 'x'), 2),\n",
              " (('f', 'k'), 2),\n",
              " (('b', 't'), 2),\n",
              " (('j', 'n'), 2),\n",
              " (('k', 'c'), 2),\n",
              " (('z', 'k'), 2),\n",
              " (('s', 'j'), 2),\n",
              " (('s', 'f'), 2),\n",
              " (('z', 'j'), 2),\n",
              " (('n', 'q'), 2),\n",
              " (('f', 'z'), 2),\n",
              " (('h', 'g'), 2),\n",
              " (('w', 'w'), 2),\n",
              " (('k', 'j'), 2),\n",
              " (('j', 'k'), 2),\n",
              " (('w', 'm'), 2),\n",
              " (('z', 'c'), 2),\n",
              " (('z', 'v'), 2),\n",
              " (('w', 'f'), 2),\n",
              " (('q', 'm'), 2),\n",
              " (('k', 'z'), 2),\n",
              " (('j', 'j'), 2),\n",
              " (('z', 'p'), 2),\n",
              " (('j', 't'), 2),\n",
              " (('k', 'b'), 2),\n",
              " (('m', 'w'), 2),\n",
              " (('h', 'f'), 2),\n",
              " (('c', 'g'), 2),\n",
              " (('t', 'f'), 2),\n",
              " (('h', 'c'), 2),\n",
              " (('q', 'o'), 2),\n",
              " (('k', 'd'), 2),\n",
              " (('k', 'v'), 2),\n",
              " (('s', 'g'), 2),\n",
              " (('z', 'd'), 2),\n",
              " (('q', 'r'), 1),\n",
              " (('d', 'z'), 1),\n",
              " (('p', 'j'), 1),\n",
              " (('q', 'l'), 1),\n",
              " (('p', 'f'), 1),\n",
              " (('q', 'e'), 1),\n",
              " (('b', 'c'), 1),\n",
              " (('c', 'd'), 1),\n",
              " (('m', 'f'), 1),\n",
              " (('p', 'n'), 1),\n",
              " (('w', 'b'), 1),\n",
              " (('p', 'c'), 1),\n",
              " (('h', 'p'), 1),\n",
              " (('f', 'h'), 1),\n",
              " (('b', 'j'), 1),\n",
              " (('f', 'g'), 1),\n",
              " (('z', 'g'), 1),\n",
              " (('c', 'p'), 1),\n",
              " (('p', 'k'), 1),\n",
              " (('p', 'm'), 1),\n",
              " (('x', 'n'), 1),\n",
              " (('s', 'q'), 1),\n",
              " (('k', 'f'), 1),\n",
              " (('m', 'k'), 1),\n",
              " (('x', 'h'), 1),\n",
              " (('g', 'f'), 1),\n",
              " (('v', 'b'), 1),\n",
              " (('j', 'p'), 1),\n",
              " (('g', 'z'), 1),\n",
              " (('v', 'd'), 1),\n",
              " (('d', 'b'), 1),\n",
              " (('v', 'h'), 1),\n",
              " (('h', 'h'), 1),\n",
              " (('g', 'v'), 1),\n",
              " (('d', 'q'), 1),\n",
              " (('x', 'b'), 1),\n",
              " (('w', 'z'), 1),\n",
              " (('h', 'q'), 1),\n",
              " (('j', 'b'), 1),\n",
              " (('x', 'm'), 1),\n",
              " (('w', 'g'), 1),\n",
              " (('t', 'b'), 1),\n",
              " (('z', 'x'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sorted(b.items(), key = lambda kv: -kv[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6G7LPewVi46C"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gZhrM-pOi46D"
      },
      "outputs": [],
      "source": [
        "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8NDBWtcPi46D"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVGIDziHi46D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gEHIO6tAi46E"
      },
      "outputs": [],
      "source": [
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  # print(chs, chs[1:])\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    N[ix1, ix2, ix3] += 1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZchLilLkj51",
        "outputId": "86625d50-f4f9-4f9d-85d4-2555a7a90394"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0, 207, 190,  31, 366,  55,  21,  17,  91, 154,  27,  75, 632, 384,\n",
              "        623,  10,  17,   9, 482, 194,  72, 152, 243,   6,  27, 173, 152],\n",
              "       dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = N[0][1].float()    #tri-gram, so it takes 2 letters or 2 int here.\n",
        "p = p / p.sum()\n",
        "p.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xlsDm7IkoJM",
        "outputId": "8be643e3-c7fa-4583-d6ef-0e371eaa9f2c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "itos[ix]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SRE3xuomk4LE",
        "outputId": "bb4ae834-1ced-48cd-d14f-60cb2257554f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "p = torch.rand(3, generator=g)\n",
        "p = p / p.sum()\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es-8KaoHk7jh",
        "outputId": "3956876d-4514-42f4-9c4c-1c5a74be8724"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6064, 0.3033, 0.0903])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = (N+1).float()\n",
        "P /= P.sum(2, keepdims=True)"
      ],
      "metadata": {
        "id": "ZrZXcgWilDq0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "    out = []\n",
        "    ix, jx = 0, 0\n",
        "    while True:\n",
        "      p = P[ix][jx]\n",
        "      next_letter = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "      ix = jx\n",
        "      jx = next_letter\n",
        "      out.append(itos[next_letter])\n",
        "      if next_letter == 0:\n",
        "        break\n",
        "    print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB5Q8QdglPkW",
        "outputId": "10815d29-7430-4119-85ef-f8f6ada5a9cd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quia.\n",
            "yu.\n",
            "quinslyntien.\n",
            "nolliahi.\n",
            "ha.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ytsj4kbi46I"
      },
      "outputs": [],
      "source": [
        "# GOAL: maximize likelihood of the data w.r.t. model parameters (statistical modeling)\n",
        "# equivalent to maximizing the log likelihood (because log is monotonic)\n",
        "# equivalent to minimizing the negative log likelihood\n",
        "# equivalent to minimizing the average negative log likelihood\n",
        "\n",
        "# log(a*b*c) = log(a) + log(b) + log(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt1x7s9si46I",
        "outputId": "615d24ba-f9da-4126-acb2-79449e1ebb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-410414.96875\n",
            "410414.96875\n",
            "2.092747449874878\n"
          ]
        }
      ],
      "source": [
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "\n",
        "for w in words:\n",
        "#for w in [\"andrejq\"]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    prob = P[ix1, ix2, ix3]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    n += 1\n",
        "    #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
        "\n",
        "print(f'{log_likelihood}')\n",
        "nll = -log_likelihood\n",
        "print(f'{nll}')\n",
        "print(f'{nll/n}')          #orginal was 2.476470470428467, so this is an improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knO-j-uLi46J",
        "outputId": "683f343c-742d-41e1-c8b7-00ef66568ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orginal word:  ['emma']\n",
            ". e m\n",
            "e m m\n",
            "m m a\n",
            "m a .\n"
          ]
        }
      ],
      "source": [
        "# create the training set of tr-igrams (x,y)\n",
        "xs, ys = [], []\n",
        "print(\"Orginal word: \", words[:1])\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    print(ch1, ch2, ch3)\n",
        "    xs.append([ix1, ix2])\n",
        "    ys.append(ix3)\n",
        "\n",
        "    \n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jN3DOUVi46J",
        "outputId": "2519cde9-815d-446b-c44a-a6d590a05912"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  5],\n",
              "        [ 5, 13],\n",
              "        [13, 13],\n",
              "        [13,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVtVfXcbi46J",
        "outputId": "38a50885-139e-4e04-eac0-b5111ca7bfca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13, 13,  1,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8gPsDXAi46K",
        "outputId": "b857ab68-c8ef-423a-da27-e2bd80139f54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes=27).float()\n",
        "xenc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOXSoPnTi46K",
        "outputId": "4d4cd7e2-985e-47a5-ac21-a7c8369fce69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "xenc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ONkUFdpi46K"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(xenc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fQyWRWXi46K",
        "outputId": "2f9089f2-0234-4368-9e64-60083c10c219"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.Size([4, 2, 27]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "xenc.dtype, xenc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAhs4-r8i46L",
        "outputId": "17007df6-8c2e-4d89-d031-166a301cb552"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "W = torch.randn((27*2, 1))\n",
        "out1 = xenc.view(4, -1) @ W\n",
        "out1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK3UGCS3i46L",
        "outputId": "cdc06692-c9a5-4cb1-9538-cba282e24946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "logits = xenc.view(4, -1) @ W # log-counts\n",
        "counts = logits.exp() # equivalent N\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GKqWrgwi46L",
        "outputId": "35d81e7e-5aee-4307-eb18-11a16cc2d5e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "probs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX4Je_Bci46L",
        "outputId": "17c1eb72-d6f3-478b-ef0f-48fa83d21765"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "probs[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3fc_eRsi46L",
        "outputId": "b4ca1e0d-446e-4dc0-8946-1b6f34b0efda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "probs[0].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55xBd-7qi46M"
      },
      "outputs": [],
      "source": [
        "# (5, 27) @ (27, 27) -> (5, 27)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZDC2CLyi46M"
      },
      "outputs": [],
      "source": [
        "# SUMMARY ------------------------------>>>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxguYZaOi46M",
        "outputId": "e2748291-450d-4df5-cbb0-34a6784c0479"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  5],\n",
              "        [ 5, 13],\n",
              "        [13, 13],\n",
              "        [13,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM838g5ei46M",
        "outputId": "f43c9b18-aaa2-4373-c051-8be1284ce3c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13, 13,  1,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "B5NV-bsUi46M"
      },
      "outputs": [],
      "source": [
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27*2, 27), generator=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "15mHv4QVi46M"
      },
      "outputs": [],
      "source": [
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc.view(-1, 27*2) @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "# btw: the last 2 lines here are together called a 'softmax'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOARxGUUi46M",
        "outputId": "dd823368-b681-401c-9161-0f52a807a22f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hs5YU3zi46N",
        "outputId": "1b67192b-4ec8-4f38-de1d-6af2247a8087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "trigram example 1: . e m (indexes 0,5,13)\n",
            "input to the neural net: 0 5\n",
            "output probabilities from the neural net: tensor([0.0237, 0.0177, 0.0107, 0.0049, 0.0223, 0.0096, 0.0111, 0.0090, 0.0071,\n",
            "        0.0424, 0.0704, 0.0511, 0.0196, 0.0240, 0.2683, 0.0824, 0.0320, 0.0058,\n",
            "        0.1061, 0.0203, 0.0267, 0.0060, 0.0026, 0.0565, 0.0026, 0.0264, 0.0407])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the the correct character: 0.023988453671336174\n",
            "log likelihood: -3.730182647705078\n",
            "negative log likelihood: 3.730182647705078\n",
            "--------\n",
            "trigram example 2: e m m (indexes 5,13,13)\n",
            "input to the neural net: 5 13\n",
            "output probabilities from the neural net: tensor([0.0224, 0.0482, 0.0110, 0.0675, 0.0691, 0.0033, 0.0108, 0.0058, 0.0093,\n",
            "        0.0043, 0.1095, 0.0867, 0.0058, 0.0074, 0.0009, 0.0084, 0.0135, 0.0044,\n",
            "        0.0651, 0.0046, 0.0327, 0.0632, 0.0720, 0.0006, 0.0040, 0.0079, 0.2616])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the the correct character: 0.0073710354045033455\n",
            "log likelihood: -4.9101972579956055\n",
            "negative log likelihood: 4.9101972579956055\n",
            "--------\n",
            "trigram example 3: m m a (indexes 13,13,1)\n",
            "input to the neural net: 13 13\n",
            "output probabilities from the neural net: tensor([0.0226, 0.0417, 0.0201, 0.0403, 0.0219, 0.0021, 0.0284, 0.0040, 0.1099,\n",
            "        0.0022, 0.0109, 0.2800, 0.0062, 0.0102, 0.0011, 0.0028, 0.0253, 0.0110,\n",
            "        0.0432, 0.0012, 0.0131, 0.0155, 0.0178, 0.0045, 0.0386, 0.1600, 0.0655])\n",
            "label (actual next character): 1\n",
            "probability assigned by the net to the the correct character: 0.04169924929738045\n",
            "log likelihood: -3.177272081375122\n",
            "negative log likelihood: 3.177272081375122\n",
            "--------\n",
            "trigram example 4: m a . (indexes 13,1,0)\n",
            "input to the neural net: 13 1\n",
            "output probabilities from the neural net: tensor([0.0104, 0.0781, 0.1008, 0.0187, 0.1996, 0.0441, 0.0178, 0.0055, 0.1757,\n",
            "        0.0179, 0.0007, 0.0651, 0.0027, 0.0142, 0.0122, 0.0315, 0.0151, 0.0067,\n",
            "        0.0196, 0.0006, 0.0230, 0.0011, 0.0198, 0.0236, 0.0645, 0.0272, 0.0040])\n",
            "label (actual next character): 0\n",
            "probability assigned by the net to the the correct character: 0.01042395830154419\n",
            "log likelihood: -4.563648223876953\n",
            "negative log likelihood: 4.563648223876953\n",
            "=========\n",
            "average negative log likelihood, i.e. loss = 4.095324993133545\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nlls = torch.zeros(4)\n",
        "for i in range(4):\n",
        "  # i-th trigram:\n",
        "  x1 = xs[i][0].item() # input character index\n",
        "  x2 = xs[i][1].item() # input character index\n",
        "  y = ys[i].item() # label character index\n",
        "  print('--------')\n",
        "  print(f'trigram example {i+1}: {itos[x1]} {itos[x2]} {itos[y]} (indexes {x1},{x2},{y})')\n",
        "  print('input to the neural net:', x1, x2)\n",
        "  print('output probabilities from the neural net:', probs[i])\n",
        "  print('label (actual next character):', y)\n",
        "  p = probs[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "\n",
        "print('=========')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "_pTlg9Hoi46N"
      },
      "outputs": [],
      "source": [
        "# --------- !!! OPTIMIZATION !!! yay --------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA2lBODRi46N",
        "outputId": "72550bcf-9bd9-4392-813a-402985c822bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  5],\n",
              "        [ 5, 13],\n",
              "        [13, 13],\n",
              "        [13,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ddBekZHi46N",
        "outputId": "31c15bd8-5d3a-4f89-9021-4233d39edf89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13, 13,  1,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bwB_Mbbbi46N"
      },
      "outputs": [],
      "source": [
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QDU5xLfui46O"
      },
      "outputs": [],
      "source": [
        "# forward pass\n",
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc.view(4, 2*27) @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "loss = -probs[torch.arange(4), ys].log().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd447CRci46O",
        "outputId": "1a0965c3-1942-4d7a-b0b2-b28a6817f9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.095324993133545\n"
          ]
        }
      ],
      "source": [
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Cna1ehgQi46O"
      },
      "outputs": [],
      "source": [
        "# backward pass\n",
        "W.grad = None # set to zero the gradient\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "arYp8z6Ui46O"
      },
      "outputs": [],
      "source": [
        "W.data += -0.1 * W.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "MuvEmTN1i46O"
      },
      "outputs": [],
      "source": [
        "# --------- !!! OPTIMIZATION !!! yay, but this time actually --------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "S0rwdEK6i46O"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append([ix1, ix2])\n",
        "    ys.append(ix3)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gROg4lICi46P",
        "outputId": "9f61871b-e6ac-49a0-bf5c-2b19079f2fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2735965251922607\n",
            "2.2732996940612793\n",
            "2.273008346557617\n",
            "2.272723436355591\n"
          ]
        }
      ],
      "source": [
        "# gradient descent\n",
        "for k in range(100):\n",
        "  \n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  logits = xenc.view(-1, 27*2) @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01*(W**2).mean()    #divided by 2 because we have a bigram here\n",
        "  if(k>95):\n",
        "    print(loss.item())\n",
        "  \n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "  \n",
        "  # update\n",
        "  W.data += -50 * W.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZtBuVfui46P"
      },
      "source": [
        "Exercises 1:\n",
        "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
        "\n",
        "As we can see, the orginal one has final loss of 2.46, and this one has 2.27. We do have improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhiIcq4Bi46Q",
        "outputId": "66f94646-6e00-4690-c554-6c8684bf1f69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "t1 = F.one_hot(xs, num_classes=27).float()\n",
        "t1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "_-YIRwCXi46W"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ67ouyqi46W",
        "outputId": "ae538ad5-131f-498e-b332-c7bb17e26b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rtn.\n",
            "ato.\n",
            "hiheyhsnyear.\n",
            "ineikahrae.\n",
            "ae.\n"
          ]
        }
      ],
      "source": [
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "  \n",
        "  out = []\n",
        "  ix = 0\n",
        "  iy = random.randint(1, 26)\n",
        "  while True:\n",
        "    \n",
        "    # ----------\n",
        "    # BEFORE:\n",
        "    #p = P[ix]\n",
        "    # ----------\n",
        "    # NOW:\n",
        "    xenc = F.one_hot(torch.tensor([ix, iy]), num_classes=27).float()\n",
        "    logits = xenc.view(-1, 27*2) @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    # ----------\n",
        "    \n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmUfYHdhi46W"
      },
      "source": [
        "Exercises 2:\n",
        "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax8FE57Ui46d"
      },
      "source": [
        "Part1: bigram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jj0Ql-Nli46e"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "GbCfw6Rhi46e"
      },
      "outputs": [],
      "source": [
        "total = len(xs)\n",
        "train_end, dev_end = int(total*0.8), int(total*0.9)\n",
        "Xtr, Ytr= xs[:train_end],  ys[:train_end]\n",
        "Xdev, Ydev= xs[train_end:dev_end],  ys[train_end:dev_end]\n",
        "Xtest, Ytest= xs[dev_end:],  ys[dev_end:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "bpSD74sAi46e"
      },
      "outputs": [],
      "source": [
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W_bgram = torch.randn((27, 27), generator=g, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApeQtP6yi46e",
        "outputId": "62ea6ca8-8546-4519-8901-3a369a66e861"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([182516, 27]), torch.Size([22815]), torch.Size([22815]))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "F.one_hot(Xtr, num_classes=27).float().shape, Ydev.shape, Ytest.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POzj9TVvi46e",
        "outputId": "100e15c8-9eb1-424d-a835-e7f6645e376f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.4629757404327393 and the dev set loss: 2.6152591705322266\n",
            "Training loss: 2.462754726409912 and the dev set loss: 2.6149585247039795\n",
            "Training loss: 2.4625394344329834 and the dev set loss: 2.6146652698516846\n",
            "Training loss: 2.4623303413391113 and the dev set loss: 2.6143786907196045\n",
            "Training loss: 2.4621260166168213 and the dev set loss: 2.614098072052002\n",
            "Training loss: 2.4619266986846924 and the dev set loss: 2.6138243675231934\n",
            "Training loss: 2.461733102798462 and the dev set loss: 2.613556385040283\n",
            "Training loss: 2.4615445137023926 and the dev set loss: 2.6132943630218506\n",
            "Training loss: 2.461360216140747 and the dev set loss: 2.6130383014678955\n",
            "Training loss: 2.4611802101135254 and the dev set loss: 2.6127872467041016\n",
            "Mean of the last 10 training loss:  2.4620471000671387\n",
            "Mean of the last 10 dev set loss:  2.613986039161682\n"
          ]
        }
      ],
      "source": [
        "last10_train_loss = []\n",
        "last10_dev_loss = []\n",
        "# gradient descent\n",
        "for k in range(100):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(Xtr, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  logits = xenc @ W_bgram  # doing the prediction\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  \n",
        "  loss_tr = -probs[torch.arange(Ytr.shape[0]), Ytr].log().mean() + 0.01 * (W_bgram**2).mean() \n",
        "  if(k>=90):\n",
        "\n",
        "    #the dev loss part\n",
        "    with torch.no_grad():\n",
        "      xdenc = F.one_hot(Xdev, num_classes=27).float() # input to the network: one-hot encoding\n",
        "\n",
        "      logits_dev = xdenc @ W_bgram  # doing the prediction\n",
        "      counts_dev = logits_dev.exp()\n",
        "      probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
        "\n",
        "      # I am not going to put the L2 regularization on dev set\n",
        "      loss_dev = -probs_dev[torch.arange(Ydev.shape[0]), Ydev].log().mean()\n",
        "      \n",
        "    print(f'Training loss: {loss_tr} and the dev set loss: {loss_dev}')\n",
        "    last10_train_loss.append(loss_tr.item())\n",
        "    last10_dev_loss.append(loss_dev.item())\n",
        "  # backward pass\n",
        "  W_bgram.grad = None # set to zero the gradient\n",
        "  # print(loss)\n",
        "  loss_tr.backward()\n",
        "\n",
        "  # update\n",
        "  W_bgram.data +=-50 * W_bgram.grad\n",
        "print(\"Mean of the last 10 training loss: \", sum(last10_train_loss)/10)\n",
        "print(\"Mean of the last 10 dev set loss: \", sum(last10_dev_loss)/10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOV5bAVni46f"
      },
      "source": [
        "This is a good result for bi-gram model. 2.4 and 2.61 are very close to each others, so this may be the case of underfitting. But the 0.2 it not really small, so I think it is not really a underfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI2VDTfMi46h",
        "outputId": "62d0578d-d485-4f7d-a269-5505927ab612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of the 10 test loss:  2.616680145263672\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  test_loss = []\n",
        "  # gradient descent\n",
        "  for k in range(10):\n",
        "    # forward pass\n",
        "    xenc = F.one_hot(Xtest, num_classes=27).float() # input to the network: one-hot encoding\n",
        "    logits = xenc @ W_bgram  # doing the prediction\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "\n",
        "    loss = -probs[torch.arange(Ytest.shape[0]), Ytest].log().mean()\n",
        "    test_loss.append(loss.item())\n",
        "\n",
        "  print(\"Mean of the 10 test loss: \", sum(test_loss)/10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2HLpzSoi46i"
      },
      "source": [
        "The dev and test splits are very close to each other.\n",
        "\n",
        "Part 2 trigram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "trQvBg_8i46i"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append([ix1, ix2])\n",
        "    ys.append(ix3)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "v5QXkpqsi46i"
      },
      "outputs": [],
      "source": [
        "total = len(xs)\n",
        "train_end, dev_end = int(total*0.8), int(total*0.9)\n",
        "Xtr, Ytr= xs[:train_end],  ys[:train_end]\n",
        "Xdev, Ydev= xs[train_end:dev_end],  ys[train_end:dev_end]\n",
        "Xtest, Ytest= xs[dev_end:],  ys[dev_end:]\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W_trigram = torch.randn((27*2, 27), generator=g, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZe15b0Ii46j",
        "outputId": "095a7393-aa1f-430f-be66-b1428d9f6b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.2377688884735107 and the dev set loss: 2.4524121284484863\n",
            "Training loss: 2.2374215126037598 and the dev set loss: 2.4520208835601807\n",
            "Training loss: 2.23708176612854 and the dev set loss: 2.451639175415039\n",
            "Training loss: 2.2367496490478516 and the dev set loss: 2.451265335083008\n",
            "Training loss: 2.2364249229431152 and the dev set loss: 2.4509003162384033\n",
            "Training loss: 2.236107110977173 and the dev set loss: 2.4505434036254883\n",
            "Training loss: 2.2357964515686035 and the dev set loss: 2.4501943588256836\n",
            "Training loss: 2.235492467880249 and the dev set loss: 2.449852705001831\n",
            "Training loss: 2.235194444656372 and the dev set loss: 2.449518918991089\n",
            "Training loss: 2.234903335571289 and the dev set loss: 2.4491918087005615\n",
            "Mean of the last 10 training loss:  2.2362940549850463\n",
            "Mean of the last 10 dev set loss:  2.450753903388977\n"
          ]
        }
      ],
      "source": [
        "last10_train_loss = []\n",
        "last10_dev_loss = []\n",
        "# gradient descent\n",
        "for k in range(100):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(Xtr, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  logits = xenc.view(-1, 27*2) @ W_trigram  # doing the prediction\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  \n",
        "  loss_tr = -probs[torch.arange(Ytr.shape[0]), Ytr].log().mean() + 0.01 * (W_trigram**2).mean() \n",
        "  if(k>=90):\n",
        "\n",
        "    #the dev loss part\n",
        "    with torch.no_grad():\n",
        "      xdenc = F.one_hot(Xdev, num_classes=27).float() # input to the network: one-hot encoding\n",
        "\n",
        "      logits_dev = xdenc.view(-1, 27*2) @ W_trigram  # doing the prediction\n",
        "      counts_dev = logits_dev.exp()\n",
        "      probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
        "\n",
        "      # I am not going to put the L2 regularization on dev set\n",
        "      loss_dev = -probs_dev[torch.arange(Ydev.shape[0]), Ydev].log().mean()\n",
        "      \n",
        "    print(f'Training loss: {loss_tr} and the dev set loss: {loss_dev}')\n",
        "    last10_train_loss.append(loss_tr.item())\n",
        "    last10_dev_loss.append(loss_dev.item())\n",
        "  # backward pass\n",
        "  W_trigram.grad = None # set to zero the gradient\n",
        "  # print(loss)\n",
        "  loss_tr.backward()\n",
        "\n",
        "  # update\n",
        "  W_trigram.data +=-50 * W_trigram.grad\n",
        "print(\"Mean of the last 10 training loss: \", sum(last10_train_loss)/10)\n",
        "print(\"Mean of the last 10 dev set loss: \", sum(last10_dev_loss)/10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-2XYCxgi46j",
        "outputId": "3c6d8044-712d-43d0-ba27-c92d6b579f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  2.4469966888427734\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  # gradient descent\n",
        "\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(Xtest, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  logits = xenc.view(-1, 27*2) @ W_trigram  # doing the prediction\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "\n",
        "  loss = -probs[torch.arange(Ytest.shape[0]), Ytest].log().mean()\n",
        "  print(\"Test loss: \", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7n5Ev_Ti46j"
      },
      "source": [
        "We can see we have a similar behavior on tri-gram and bi-gram, which the training loss has different of 0.2.\n",
        "We can see the tri-gram is better than the bi-gram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5X9mJ24i46j"
      },
      "source": [
        "Exercises 3:\n",
        "Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cVC9TZS7i46j"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUnu4tlHi46k",
        "outputId": "3233d228-1a80-4868-b0be-23a0642aa135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------with smooting value of 1-------------\n",
            "Mean of training loss:  2.605528769493103\n",
            "Mean of dev set loss:  2.587269995212555\n",
            "----------------------------\n",
            "---------with smooting value of 0.5-------------\n",
            "Mean of training loss:  2.515159344673157\n",
            "Mean of dev set loss:  2.557986702919006\n",
            "----------------------------\n",
            "---------with smooting value of 0.1-------------\n",
            "Mean of training loss:  2.3942275929450987\n",
            "Mean of dev set loss:  2.5475563192367554\n",
            "----------------------------\n",
            "---------with smooting value of 0.05-------------\n",
            "Mean of training loss:  2.368103873729706\n",
            "Mean of dev set loss:  2.5511179089546205\n",
            "----------------------------\n",
            "---------with smooting value of 0.01-------------\n",
            "Mean of training loss:  2.3415348505973816\n",
            "Mean of dev set loss:  2.556204407215118\n",
            "----------------------------\n",
            "---------with smooting value of 0.005-------------\n",
            "Mean of training loss:  2.337708020210266\n",
            "Mean of dev set loss:  2.557020697593689\n",
            "----------------------------\n",
            "---------with smooting value of 0.001-------------\n",
            "Mean of training loss:  2.334548394680023\n",
            "Mean of dev set loss:  2.557706644535065\n",
            "----------------------------\n",
            "---------with smooting value of 0.0005-------------\n",
            "Mean of training loss:  2.3341470694541933\n",
            "Mean of dev set loss:  2.5577945208549497\n",
            "----------------------------\n",
            "---------with smooting value of 0.0001-------------\n",
            "Mean of training loss:  2.33382493019104\n",
            "Mean of dev set loss:  2.55786518573761\n",
            "----------------------------\n",
            "---------with smooting value of 5e-05-------------\n",
            "Mean of training loss:  2.333784601688385\n",
            "Mean of dev set loss:  2.557873980998993\n",
            "----------------------------\n",
            "---------with smooting value of 1e-05-------------\n",
            "Mean of training loss:  2.3337523102760316\n",
            "Mean of dev set loss:  2.5578811240196226\n",
            "----------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "regs=[1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
        "\n",
        "\n",
        "for r in regs:\n",
        "    # # initialize the 'network'\n",
        "    g = torch.Generator().manual_seed(2147483647)\n",
        "    W_trigram = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
        "    print(f\"---------with smooting value of {r}-------------\")\n",
        "    train_loss = []\n",
        "    dev_loss = []\n",
        "    # gradient descent\n",
        "    for k in range(100):\n",
        "        # forward pass\n",
        "        xenc = F.one_hot(Xtr, num_classes=27).float() # input to the network: one-hot encoding\n",
        "        logits = xenc.view(-1, 27*2) @ W_trigram  # doing the prediction\n",
        "        counts = logits.exp()\n",
        "        probs = counts / counts.sum(1, keepdims=True)\n",
        "        \n",
        "        loss_tr = -probs[torch.arange(Ytr.shape[0]), Ytr].log().mean() + r * (W_trigram**2).mean() \n",
        "\n",
        "\n",
        "        #the dev loss part\n",
        "        with torch.no_grad():\n",
        "            xdenc = F.one_hot(Xdev, num_classes=27).float() # input to the network: one-hot encoding\n",
        "\n",
        "            logits_dev = xdenc.view(-1, 27*2) @ W_trigram  # doing the prediction\n",
        "            counts_dev = logits_dev.exp()\n",
        "            probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
        "\n",
        "            # I am not going to put the L2 regularization on dev set\n",
        "            loss_dev = -probs_dev[torch.arange(Ydev.shape[0]), Ydev].log().mean()\n",
        "            \n",
        "        train_loss.append(loss_tr.item())\n",
        "        dev_loss.append(loss_dev.item())\n",
        "\n",
        "        # backward pass\n",
        "        W_trigram.grad = None # set to zero the gradient\n",
        "        loss_tr.backward()\n",
        "\n",
        "        # update\n",
        "        W_trigram.data +=-50 * W_trigram.grad\n",
        "    print(\"Mean of training loss: \", sum(train_loss)/len(train_loss))\n",
        "    print(\"Mean of dev set loss: \", sum(dev_loss)/len(dev_loss))\n",
        "    print(\"----------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D0nX70Wi46k"
      },
      "source": [
        "---------with smooting value of 1-------------\n",
        "Mean of training loss:  2.6055288100242615\n",
        "Mean of dev set loss:  2.5872700142860414\n",
        "----------------------------\n",
        "---------with smooting value of 0.5-------------\n",
        "Mean of training loss:  2.5151593542099\n",
        "Mean of dev set loss:  2.557986719608307\n",
        "----------------------------\n",
        "---------with smooting value of 0.1-------------\n",
        "Mean of training loss:  2.3942275857925415\n",
        "Mean of dev set loss:  2.5475563406944275\n",
        "----------------------------\n",
        "---------with smooting value of 0.05-------------\n",
        "Mean of training loss:  2.368103895187378\n",
        "Mean of dev set loss:  2.5511179327964784\n",
        "----------------------------\n",
        "---------with smooting value of 0.01-------------\n",
        "Mean of training loss:  2.3415348315238953\n",
        "Mean of dev set loss:  2.5562044095993044\n",
        "----------------------------\n",
        "---------with smooting value of 0.005-------------\n",
        "Mean of training loss:  2.3377080392837524\n",
        "Mean of dev set loss:  2.5570207262039184\n",
        "----------------------------\n",
        "---------with smooting value of 0.001-------------\n",
        "Mean of training loss:  2.334548397064209\n",
        "Mean of dev set loss:  2.557706673145294\n",
        "----------------------------\n",
        "---------with smooting value of 0.0005-------------\n",
        "Mean of training loss:  2.334147057533264\n",
        "Mean of dev set loss:  2.5577945470809937\n",
        "----------------------------\n",
        "---------with smooting value of 0.0001-------------\n",
        "Mean of training loss:  2.3338249206542967\n",
        "Mean of dev set loss:  2.557865195274353\n",
        "----------------------------\n",
        "---------with smooting value of 5e-05-------------\n",
        "Mean of training loss:  2.3337846159935\n",
        "Mean of dev set loss:  2.55787401676178\n",
        "----------------------------\n",
        "---------with smooting value of 1e-05-------------\n",
        "Mean of training loss:  2.3337523221969603\n",
        "Mean of dev set loss:  2.5578811287879946\n",
        "----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Ml37G2eQi46k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9S4pNaei46k"
      },
      "source": [
        "Exercises 4:\n",
        "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX3761fYi46k",
        "outputId": "37bec890-584c-4120-dc71-687afbba0bbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([156890, 2, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "F.one_hot(Xtr, num_classes=27).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owTMlV_ui46l",
        "outputId": "492dbf59-1998-4631-c4d0-2d768f176515"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([156890, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "Xtr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTZVDUWDi46l",
        "outputId": "f8cb2844-c1c6-466a-e61e-79cb3be4dd45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "Xtr[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2AvwrDJi46l",
        "outputId": "9a284a1e-797d-4213-b2b5-84c8150b0c6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "F.one_hot(Xtr[0], num_classes=27)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "gvVccUP7i46l"
      },
      "outputs": [],
      "source": [
        "input_v = F.one_hot(Xtr[0], num_classes=27)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgfCBy5_i46l",
        "outputId": "12e6b0b8-2771-4033-c46e-32fd2fb689d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "input_v[0] +input_v[1] #simply indexing into rows of W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOiGHNKoi46m",
        "outputId": "17cc1856-b85f-48e3-aeda-ba0fd36e601a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([156890, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "(F.one_hot(Xtr, num_classes=27)[:,0]+ F.one_hot(Xtr, num_classes=27)[:,1]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9C0l4I2i46m"
      },
      "source": [
        "As a result, we just add the row together to stack them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mis2GpXGi46m"
      },
      "source": [
        "Exercise 6\n",
        "E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n",
        "Try to decrease the learning rate at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sd-fsiDi46m",
        "outputId": "7e38f860-7c96-4369-9d9b-c12b794af2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------with smooting value of 1e-05-------------\n",
            "Mean of training loss:  2.631999144554138\n",
            "Mean of dev set loss:  2.723759572505951\n"
          ]
        }
      ],
      "source": [
        "# # initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W_trigram = torch.randn((27, 27), generator=g, requires_grad=True)\n",
        "print(f\"---------with smooting value of {r}-------------\")\n",
        "train_loss = []\n",
        "dev_loss = []\n",
        "# gradient descent\n",
        "lr = 50\n",
        "for k in range(100):\n",
        "    # forward pass\n",
        "    xenc = (F.one_hot(Xtr, num_classes=27)[:,0]+ F.one_hot(Xtr, num_classes=27)[:,1]).float() # input to the network: one-hot encoding\n",
        "    logits = xenc @ W_trigram  # doing the prediction\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "    \n",
        "    loss_tr = -probs[torch.arange(Ytr.shape[0]), Ytr].log().mean() + 0.1 * (W_trigram**2).mean() \n",
        "\n",
        "\n",
        "    #the dev loss part\n",
        "    with torch.no_grad():\n",
        "        xdenc = (F.one_hot(Xdev, num_classes=27)[:,0]+ F.one_hot(Xdev, num_classes=27)[:,1]).float()# input to the network: one-hot encoding\n",
        "\n",
        "        logits_dev = xdenc @ W_trigram  # doing the prediction\n",
        "        counts_dev = logits_dev.exp()\n",
        "        probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
        "\n",
        "        # I am not going to put the L2 regularization on dev set\n",
        "        loss_dev = -probs_dev[torch.arange(Ydev.shape[0]), Ydev].log().mean()\n",
        "        \n",
        "    train_loss.append(loss_tr.item())\n",
        "    dev_loss.append(loss_dev.item())\n",
        "\n",
        "    # backward pass\n",
        "    W_trigram.grad = None # set to zero the gradient\n",
        "    loss_tr.backward()\n",
        "\n",
        "    # update\n",
        "    if(k==75):\n",
        "        lr = 25\n",
        "    W_trigram.data +=-lr * W_trigram.grad\n",
        "print(\"Mean of training loss: \", sum(train_loss)/len(train_loss))\n",
        "print(\"Mean of dev set loss: \", sum(dev_loss)/len(dev_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C76GyFwSi46m",
        "outputId": "354e8825-6ace-4b95-93bf-adea790d3d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  2.558659791946411\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  # gradient descent\n",
        "\n",
        "  # forward pass\n",
        "  xenc =(F.one_hot(Xtest, num_classes=27)[:,0]+ F.one_hot(Xtest, num_classes=27)[:,1]).float() # input to the network: one-hot encoding\n",
        "  logits = xenc@ W_trigram  # doing the prediction\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "\n",
        "  loss = -probs[torch.arange(Ytest.shape[0]), Ytest].log().mean()\n",
        "  print(\"Test loss: \", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jh_OoBiJmBVI"
      },
      "execution_count": 83,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a6e773fc7f7b2b8b91c25b211311bf7ed1bb15ad853df008c3aead80ddb88d4"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}